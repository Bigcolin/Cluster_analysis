module qgplsim
using Optim, LinearAlgebra, Distributions, Random, Statistics
function Base.:-(X::Matrix, xi::Vector)
    n, p = size(X)
    for i in 1:n
        X[i, :] = X[i, :] - xi
    end
    X
end

function collection(c)
    n,  = size(c)
    categ_c = []
    dict_c = Dict()
    for i in 1:n
        if c[i, :] in categ_c
            push!(dict_c[c[i, :]], i)
        else
            push!(categ_c, c[i, :])
            dict_c[c[i, :]] = [i]
        end
    end
    most_c = argmax(length.(dict_c[z] for z in categ_c))
    categ_c, dict_c, categ_c[most_c]
end

distribution_ker = Normal(0, 1)

function ker(x)
    pdf(distribution_ker, x)
end

function ker(x::Vector, h::Vector)
    d = length(x)
    v = 1
    for i in 1:d 
        v *= ker(x[i]/h[i])/h[i]
    end
    v
end

function ker(X::Matrix, h::Vector)
    n, p = size(X)
    v = zeros(n)
    for i in 1:n
        v[i] = ker(X[i, :], h)
    end
    v
end

ρ(x, α = 0.5) = abs(x) + (2α - 1)x

function optimfunc(f, init_value, tols = 1e-4)
    res = optimize(f, init_value, method=BFGS(), f_tol=tols)
    return res 
end


function alpha_estimator(X, Z, y)
	n, p = size(X)
	α, θ = zeros(p), zeros(p)
	categ, index, = collection(Z)
	ncateg = index.count

	sign_alpha = []
	αz = zeros(ncateg, p)
	θz = zeros(ncateg, p)
	Cz = zeros(n, p)
	Bz = zeros(n, p)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		az = zeros(nz)
		bz = zeros(nz, p)
		cz = zeros(nz, p)
		yz = y[indz]
		Xz = X[indz, :]
		KerVal = zeros(nz, nz)
		h = 2 * ones(p) .* nz^(-2 / (p + 4))
		for i in 1:nz
            Xi = Xz - Xz[i, :]
			KerVal[:, i] = ker(Xi, h)
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (Xz - Xz[i, :]) * w[2:end]) .* KerVal[:, i])
			res = optimfunc(tar_alpha, zeros(p + 1))
			wi = res.minimizer
			az[i] = wi[1]
			bz[i,:] = wi[2:end]
			Bz[indz[i],:] = wi[2:end]

		end

		sum_bz = zeros(p)
		for i in 1:nz - 1
			for j in i + 1:nz
				sum_bz += (bz[i, :] - bz[j, :])
			end
		end
		αz[k,:] = sum_bz / norm(sum_bz) # this is not alpha, or alpha / ca 

		theta_z = zeros(nz, p)
		for i in 1:nz
			tar_theta(w) = sum(ρ.(yz .- az[i] - (Xz - Xz[i, :])*(w[(p + 1):end] .+ sum(w[1:p] .* αz[k, :]))) .* KerVal[:, i])
			res = optimfunc(tar_theta, zeros(2*p))
			wi = res.minimizer
			cz[i,:] = wi[1:p]
			Cz[indz[i],:] = wi[1:p]
			theta_z[i,:] = wi[(p + 1):end]
		end
		θz[k, :] = sum(theta_z, dims = 1)/nz

		# ca = 0 # coef. of alpha
		# for i in 1:nz
		# 	ca +=  norm(bz[i, :])
		# end
		# ca = sum(abs.(cz)) / ca 
		# println(ca, "\n")
		# αz[k, :] = αz[k, :] * ca
		push!(sign_alpha, sign.(αz[k, :])) 
		α += abs.(αz[k, :]) * nz / n
		θ += θz[k, :] * nz / n
	end
	c, d, sign_alpha = collection(sign_alpha)
	α.* sign_alpha[1],  Bz # Cz

end

function theta_estimator(alpha, Bz)
	n, p = size(Bz)
	theta = sum(Bz, dims = 1) - sum(Bz * alpha) * alpha / norm(alpha)
	theta / n
end

function gamma_estimator(v, Z, y)

	n, q = size(Z)
	gamma = zeros(q)
	categ, index, = collection(Z)
	ncateg = index.count
	Gz = zeros(n)
	DGz = zeros(n)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		dgz = zeros(nz)
		gz = zeros(nz)
		yz = y[indz]
		vz = v[indz]
		KerVal = zeros(nz, nz)
		h = nz^(-0.2)
		for i in 1:nz
			KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2]) .* KerVal[:, i])
			res = optimfunc(tar_alpha, zeros(2))
			wi = res.minimizer
			dgz[i] = wi[2]
			DGz[indz[i]] = wi[2]

			gz[i] = wi[1]
			Gz[indz[i]] = wi[1]

		end
	end
	Gz, DGz

end

function beta_estimator()
	
end

end # module
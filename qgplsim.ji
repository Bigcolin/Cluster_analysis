
module qgplsim
using Optim, LinearAlgebra, Distributions, Random, Statistics
function Base.:-(X::Matrix, xi::Vector)
    n, p = size(X)
    for i in 1:n
        X[i, :] = X[i, :] - xi
    end
    X
end

function collection(c)
    n,  = size(c)
    categ_c = []
    dict_c = Dict()
    for i in 1:n
        if c[i, :] in categ_c
            push!(dict_c[c[i, :]], i)
        else
            push!(categ_c, c[i, :])
            dict_c[c[i, :]] = [i]
        end
    end
    most_c = argmax(length.(dict_c[z] for z in categ_c))
    categ_c, dict_c, categ_c[most_c]
end

distribution_ker = Normal(0, 1)

function ker(x)
    pdf(distribution_ker, x)
end

function ker(x::Vector, h::Float64)
	ker.(x/h)/h
end

function ker(x::Vector, h::Vector)
    d = length(x)
    v = 1
    for i in 1:d 
        v *= ker(x[i]/h[i])/h[i]
    end
    v
end

function ker(X::Matrix, h::Vector)
    n, p = size(X)
    v = zeros(n)
    for i in 1:n
        v[i] = ker(X[i, :], h)
    end
    v
end

ρ(x, α = 0.5) = abs(x) + (2α - 1)x

function optimfunc(f, init_value, tols = 1e-4)
    res = optimize(f, init_value, method=BFGS(), f_tol=tols)
    return res 
end
function glquad(g, v0, v1, c0, c1, ng::Int = 11)
	
	if ng == 11

	      knots = [0, -0.9782, -0.8871, -0.7302, -0.5191, -0.2695, 0.9782, 0.8871, 0.7302, 0.5191, 0.2695]
	      weigtht = [ 0.2729, 0.0557, 0.1256, 0.1863, 0.2332, 0.2628, 0.0557, 0.1256, 0.1863, 0.2332, 0.2628]

	elseif ng == 8

	      knots = [-0.4801,    -0.3983,   -0.2628,   -0.0917,    0.0917,    0.2628,    0.3983,    0.4801] .* 2
	      weigtht = [0.1012,       0.22240,   0.3137,    0.3627,    0.3627,    0.3137,    0.2224,    0.1012]         

	elseif ng == 6
		
	      knots = [-0.93246951,-0.66120939, -0.23861919,  0.23861919,  0.66120939,  0.93246951]
	      weigtht = [0.17132449,   0.36076157,  0.46791393,  0.46791393,  0.36076157,  0.17132449]     


	end

	v = (knots .+ 1)./2 .* (v1 - v0) .+ v0
	gv = g.(v) 

	gv_ = c0 .* (gv .< c0) .+ c1 .* (gv .> c1) .+ gv .* (1 .- (gv .< c0) .- (gv .> c1)) 
	transpose(gv_) * weigtht
end

struct model
	X::Matrix{Float64}
	Z::Matrix{Int}
	y::Vector{Float64}
	categ::Vector{Vector{Int}}
	index::Dict{Vector{Int}, Vector{Int}}
	# alpha::Vector{Float64}
	# theta::Vector{Float64}
	quantileNum::Float64

	function model(X, Z, y, τ = 0.5)
		categ, index, = collection(Z)
		new(X, Z, y, categ, index, τ)
	end


end

function estimator(data::model)
	alpha, dGz = alpha_estimator(data)
	theta = theta_estimator(alpha, dGz)
	gamma = gamma_estimator(data, alpha, theta)
	alpha, theta, gamma
end

function alpha_estimator(data::model)
	X, Z, y, τ = data.X, data.Z, data.y, data.quantileNum
	
	n, p = size(X)
	α, θ = zeros(p), zeros(p)
	categ, index = data.categ, data.index
	ncateg = index.count

	sign_alpha = []
	αz = zeros(ncateg, p)
	θz = zeros(ncateg, p)
	Cz = zeros(n, p)
	Bz = zeros(n, p)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		az = zeros(nz)
		bz = zeros(nz, p)
		cz = zeros(nz, p)
		yz = y[indz]
		Xz = X[indz, :]
		KerVal = zeros(nz, nz)
		h = 2 * ones(p) .* nz^(-2 / (p + 4))
		for i in 1:nz
            Xi = Xz - Xz[i, :]
			KerVal[:, i] = ker(Xi, h)
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (Xz - Xz[i, :]) * w[2:end], τ) .* KerVal[:, i])
			res = optimfunc(tar_alpha, zeros(p + 1))
			wi = res.minimizer
			az[i] = wi[1]
			bz[i,:] = wi[2:end]
			Bz[indz[i],:] = wi[2:end]

		end

		sum_bz = zeros(p)
		for i in 1:nz - 1
			for j in i + 1:nz
				sum_bz += (bz[i, :] - bz[j, :])
			end
		end
		αz[k,:] = sum_bz / norm(sum_bz) # this is not alpha, or alpha / ca 

		theta_z = zeros(nz, p)
		for i in 1:nz
			tar_theta(w) = sum(ρ.(yz .- az[i] - (Xz - Xz[i, :])*(w[(p + 1):end] .+ sum(w[1:p] .* αz[k, :])), τ) .* KerVal[:, i])
			res = optimfunc(tar_theta, zeros(2*p))
			wi = res.minimizer
			cz[i,:] = wi[1:p]
			Cz[indz[i],:] = wi[1:p]
			theta_z[i,:] = wi[(p + 1):end]
		end
		θz[k, :] = sum(theta_z, dims = 1)/nz

		# ca = 0 # coef. of alpha
		# for i in 1:nz
		# 	ca +=  norm(bz[i, :])
		# end
		# ca = sum(abs.(cz)) / ca 
		# println(ca, "\n")
		# αz[k, :] = αz[k, :] * ca
		push!(sign_alpha, sign.(αz[k, :])) 
		α += abs.(αz[k, :]) * nz / n
		θ += θz[k, :] * nz / n
	end
	c, d, sign_alpha = collection(sign_alpha)

	alpha = α.* sign_alpha[1]
	if sum(alpha) < 0
		alpha = -alpha
	end
	alpha,  Bz # Cz

end

function theta_estimator(alpha, Bz)
	n, p = size(Bz)
	theta = sum(Bz, dims = 1) - sum(Bz * alpha) * alpha / norm(alpha)
	theta / n
end

function dgz(v, vz, yz, τ)
	nz = length(vz)
	h = 2*nz^(-0.4)
	KerVal = ker(v .- vz, h)
	tar_dgz(w) = sum(ρ.(yz .- w[1] - (v .- vz) * w[2], τ) .* KerVal)
	res = optimfunc(tar_dgz, zeros(2))
	wi = res.minimizer
	dgz = wi[2]

	# gz= wi[1]
	dgz
end

function gamma_estimator(data::model, alpha, theta)
	v = data.X * alpha
	y = data.y - data.X * transpose(theta)
	τ = data.quantileNum
	categ, index = data.categ, data.index
	ncateg = index.count
	ΔJ = zeros(ncateg - 1)
	ΔZ = transpose(hcat(categ...))
	ΔZ = ΔZ[2:end, :] - ΔZ[1, :]
	v0, v1 = -1e5, 1e5
	C0 = zeros(ncateg)
	C1 = zeros(ncateg)
	dg(u, k) = dgz(u, v[index[categ[k]]], y[index[categ[k]]], τ)

	for k in 1:ncateg
		vk = v[index[categ[k]]]
		vmin = minimum(vk)
		vmax = maximum(vk)
		if v0 < vmin
			v0 = vmin
		end

		if v1 > vmax
			v1 = vmax
		end
	end

	for k in 1:ncateg
		C0[k] = dg(v0, k)
		C1[k] = dg(v1, k)
	end
	c0 = minimum(C0)
	c1 = maximum(C1)

	dg1(u) = dg(u, 1)
	ΔJ = ΔJ .- glquad(dg1, v0, v1, c0, c1)
	for k in 1:ncateg - 1
		dgk(u) = dg(u, k + 1)
		ΔJ[k] += glquad(dgk, v0, v1, c0, c1)
	end
	gamma = inv(transpose(ΔZ)*ΔZ) * transpose(ΔZ) * ΔJ /(c1 - c0)
	if sum(gamma) < 0
		gamma = - gamma
	end
	gamma

end

function Gz_DGz(v, Z, y)

	n, q = size(Z)
	categ, index, = collection(Z)
	ncateg = index.count
	Gz = zeros(n)
	DGz = zeros(n)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		dgz = zeros(nz)
		gz = zeros(nz)
		yz = y[indz]
		vz = v[indz]
		KerVal = zeros(nz, nz)
		h = 2*nz^(-0.4)
		for i in 1:nz
			KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2]) .* KerVal[:, i])
			res = optimfunc(tar_alpha, zeros(2))
			wi = res.minimizer
			dgz[i] = wi[2]
			DGz[indz[i]] = wi[2]

			gz[i] = wi[1]
			Gz[indz[i]] = wi[1]

		end
	end
	Gz, DGz

end

function beta_estimator()
	
end

end # module